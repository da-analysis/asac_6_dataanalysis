import os
import pandas as pd
from langchain.document_loaders import PyMuPDFLoader, CSVLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# âœ… OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡)
os.environ["OPENAI_API_KEY"] = ""

# âœ… ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))  # `utils` í´ë”ì˜ ì ˆëŒ€ ê²½ë¡œ
base_dir = os.path.dirname(current_dir)  # `utils`ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬

# âœ… ë³€ê²½ëœ ê²½ë¡œ
pdf_directory = os.path.join(base_dir, "ì´ìš©ê°€ì´ë“œ")
csv_file_path = os.path.join(base_dir, "ë³´ì¡°ê¸ˆ24_í˜œíƒ", "ì •ë¶€24_ë³´ì¡°ê¸ˆ24_ì§€ì—­í™”í_í¬ë¡¤ë§_ì „ì²˜ë¦¬.csv")
vector_store_path = os.path.join(base_dir, "vector_store", "text_chatbot_chromadb")

# âœ… OpenAI ì„ë² ë”© ëª¨ë¸
embeddings = OpenAIEmbeddings()

# âœ… 1ï¸âƒ£ PDF ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
def load_pdf_documents(pdf_directory):
    documents = []

    if not os.path.exists(pdf_directory):
        print(f"ğŸš¨ PDF ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_directory}")
        return documents

    for filename in os.listdir(pdf_directory):
        if filename.endswith(".pdf"):
            file_path = os.path.join(pdf_directory, filename)
            loader = PyMuPDFLoader(file_path)
            doc_list = loader.load()

            # ğŸ“Œ íŒŒì¼ëª…ì—ì„œ ì§€ì—­ëª…, ì§€ì—­í™”íëª… ì¶”ì¶œ
            parts = filename.replace(".pdf", "").split("_")
            sido = parts[0] if len(parts) > 0 else "ì •ë³´ ì—†ìŒ"
            sigungu = parts[1] if len(parts) > 1 else "ì •ë³´ ì—†ìŒ"
            currency_name = parts[2] if len(parts) > 2 else "ì •ë³´ ì—†ìŒ"

            # ğŸ“Œ PDF íŒŒì¼ë³„ ìë™ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in doc_list:
                doc.metadata.update({
                    "ì‹œë„": sido,
                    "ì‹œêµ°êµ¬": sigungu,
                    "ì§€ì—­í™”íëª…": currency_name,
                    "íŒŒì¼ëª…": filename,
                    "ë¬¸ì„œìœ í˜•": "database"
                })
            documents.extend(doc_list)

    print(f"âœ… PDF ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(documents)} ê°œ")
    return documents

# âœ… 2ï¸âƒ£ CSV íŒŒì¼ ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ë³€í™˜
def load_csv_documents(csv_file_path):
    if not os.path.exists(csv_file_path):
        print(f"ğŸš¨ CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {csv_file_path}")
        return []

    df = pd.read_csv(csv_file_path, encoding="cp949")

    # âœ… ì²« 3ê°œ ì»¬ëŸ¼ì€ ë©”íƒ€ë°ì´í„°, ë‚˜ë¨¸ì§€ëŠ” ë³¸ë¬¸
    meta_columns = df.iloc[:, :3].columns.tolist()
    content_columns = df.iloc[:, 3:].columns.tolist()

    csv_documents = []
    for _, row in df.iterrows():
        metadata = {
            "ì‹œë„": row[meta_columns[0]],
            "ì‹œêµ°êµ¬": row[meta_columns[1]],
            "ì§€ì—­í™”íëª…": row[meta_columns[2]],
            "ë¬¸ì„œìœ í˜•": "database"
        }

        # âœ… ë³¸ë¬¸ ë‚´ìš© ê²°í•©
        text_content = "\n".join([f"{col}: {row[col]}" for col in content_columns])

        # âœ… CSV ë°ì´í„°ë¥¼ `Document` ê°ì²´ë¡œ ë³€í™˜
        csv_documents.append(Document(page_content=text_content, metadata=metadata))

    print(f"âœ… CSV ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(csv_documents)} ê°œ")
    return csv_documents

# âœ… 3ï¸âƒ£ ë¬¸ì„œ ë¶„í• 
def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
    split_docs = text_splitter.split_documents(documents)

    print(f"âœ… ë¬¸ì„œ ì¡°ê° ìƒì„± ì™„ë£Œ: {len(split_docs)} ê°œ")

    # âœ… ë©”íƒ€ë°ì´í„° í•„ë“œ ì œí•œ
    allowed_metadata_keys = {"ì‹œë„", "ì‹œêµ°êµ¬", "ì§€ì—­í™”íëª…", "íŒŒì¼ëª…", "ë¬¸ì„œìœ í˜•"}

    cleaned_docs = []
    for doc in split_docs:
        new_metadata = {key: doc.metadata.get(key, "ì •ë³´ ì—†ìŒ") for key in allowed_metadata_keys}
        cleaned_docs.append(Document(page_content=doc.page_content, metadata=new_metadata))

    print(f"âœ… ë©”íƒ€ë°ì´í„° ì •ë¦¬ ì™„ë£Œ: {len(cleaned_docs)} ê°œ")
    return cleaned_docs

# âœ… 4ï¸âƒ£ ChromaDBì— ì €ì¥
def save_to_chroma(split_docs, persist_directory):
    os.makedirs(persist_directory, exist_ok=True)

    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name='text_db_0'
    )

    vectorstore.persist()
    print(f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ: {persist_directory}")

# âœ… ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ (PDF + CSV í†µí•© ì €ì¥)
print("ğŸ“Œ ë°ì´í„° ë¡œë”© ì‹œì‘...")

pdf_documents = load_pdf_documents(pdf_directory)
csv_documents = load_csv_documents(csv_file_path)

# âœ… ë¬¸ì„œ í†µí•©
all_documents = pdf_documents + csv_documents

if len(all_documents) == 0:
    print("ğŸš¨ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
else:
    # âœ… ë¬¸ì„œ ë¶„í• 
    split_docs = split_documents(all_documents)

    # âœ… ChromaDB ì €ì¥
    save_to_chroma(split_docs, persist_directory=vector_store_path)

print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
